import streamlit as st
import os
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(page_title="AI é†«é™¢å¿ƒè‡Ÿç–¾ç—…å•ç­”æ©Ÿ", page_icon="ğŸ¥")

# ---------------------------
# Sidebar: API Key inputs
# ---------------------------
st.sidebar.header("ğŸ”‘ é‡‘é‘°è¨­å®š")

# Groq API Key: å¿…å¡«ï¼ˆä½¿ç”¨è€…è¼¸å…¥å¾Œæ‰å•Ÿå‹• RAGï¼‰
groq_key_input = st.sidebar.text_input(
    "Groq API Key",
    type="password",
    value=st.session_state.get("GROQ_API_KEY", ""),
    placeholder="gsk_...",
)

# (å¯é¸) HuggingFace Tokenï¼šè‹¥ä½  embeddings / æ¨¡å‹éœ€è¦ HF æ¬Šé™æ‰å¡«
hf_token_input = st.sidebar.text_input(
    "Hugging Face Token (å¯é¸)",
    type="password",
    value=st.session_state.get("HUGGING_FACE_HUB_TOKEN", ""),
    placeholder="hf_...",
)

# æŒ‰éˆ•ï¼šæ˜ç¢ºç”±ä½¿ç”¨è€…è§¸ç™¼ã€Œé–‹å§‹ã€
start = st.sidebar.button("å•Ÿå‹• / æ›´æ–°é‡‘é‘°", type="primary")

if start:
    # å­˜åˆ° session_stateï¼Œé¿å…æ¯æ¬¡ rerun éƒ½è¦é‡æ‰“
    st.session_state["GROQ_API_KEY"] = groq_key_input.strip()
    st.session_state["HUGGING_FACE_HUB_TOKEN"] = hf_token_input.strip()

# è®“ç¨‹å¼æœ¬æ¬¡åŸ·è¡Œä¹Ÿèƒ½è®€åˆ°ï¼ˆLangChain/Groq client é€šå¸¸å¾åƒæ•¸è®€ï¼Œä½†ä½ å¯èƒ½ä¹Ÿæœƒç”¨ envï¼‰
if st.session_state.get("GROQ_API_KEY"):
    os.environ["GROQ_API_KEY"] = st.session_state["GROQ_API_KEY"]

if st.session_state.get("HUGGING_FACE_HUB_TOKEN"):
    os.environ["HUGGING_FACE_HUB_TOKEN"] = st.session_state["HUGGING_FACE_HUB_TOKEN"]


# ---------------------------
# Lazy load resources (AFTER key is set)
# ---------------------------
@st.cache_resource
def load_vector_store():
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    vectorstore = FAISS.load_local(
        "faiss_db",
        embedding_model,
        allow_dangerous_deserialization=True
    )
    return vectorstore

@st.cache_resource
def load_llm(groq_api_key: str):
    return ChatGroq(
        temperature=0,
        groq_api_key=groq_api_key,
        model_name="gemma-7b-it",
    )

# ---------------------------
# Prompt template
# ---------------------------
system_prompt = "ä½ æ˜¯å¿ƒè‡Ÿç§‘çš„å¯¦ç¿’é†«ç”Ÿï¼Œè«‹æ ¹æ“šè³‡æ–™ä¾†å›æ‡‰ç—…æ‚£çš„å•é¡Œã€‚è«‹è¦ªåˆ‡ã€ç°¡æ½”ä¸¦é™„å¸¶å…·é«”å»ºè­°ã€‚è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›æ‡‰ã€‚"
prompt_template = """
æ ¹æ“šä¸‹åˆ—è³‡æ–™ï¼š
{retrieved_chunks}

å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼š{question}

è«‹æ ¹æ“šè³‡æ–™å…§å®¹å›è¦†ï¼Œè‹¥è³‡æ–™ä¸è¶³è«‹å‘Šè¨´ç—…æ‚£å¯ä»¥å‰å¾€æœ€è¿‘çš„é†«é™¢å•è¨ºã€‚
"""


# ---------------------------
# UI
# ---------------------------
st.title("ğŸ“ AI é†«é™¢å¿ƒè‡Ÿç–¾ç—…å•ç­”æ©Ÿ")

# æ²’æœ‰ key å°±å…ˆæ“‹ä½ï¼Œé¿å…è¼‰å…¥/å‘¼å«å¤±æ•—
if not st.session_state.get("GROQ_API_KEY"):
    st.info("è«‹å…ˆåœ¨å·¦å´è¼¸å…¥ Groq API Keyï¼ŒæŒ‰ã€Œå•Ÿå‹• / æ›´æ–°é‡‘é‘°ã€å¾Œå†é–‹å§‹å•ç­”ã€‚")
    st.stop()

# åˆå§‹åŒ– LLM / Retrieverï¼ˆç¢ºä¿ key å·²å­˜åœ¨ï¼‰
try:
    vectorstore = load_vector_store()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
except Exception as e:
    st.error(f"å‘é‡åº«è¼‰å…¥å¤±æ•—ï¼š{e}\nè«‹ç¢ºèª faiss_db ç›®éŒ„å­˜åœ¨ä¸”å¯è®€å–ã€‚")
    st.stop()

try:
    llm = load_llm(st.session_state["GROQ_API_KEY"])
except Exception as e:
    st.error(f"LLM åˆå§‹åŒ–å¤±æ•—ï¼š{e}\nè«‹ç¢ºèª Groq API Key æ­£ç¢ºã€‚")
    st.stop()


# ---------------------------
# Chat state
# ---------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥ä½ çš„å•é¡Œ..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        docs = retriever.invoke(prompt)
        retrieved_chunks = "\n\n".join([doc.page_content for doc in docs])

        final_prompt = prompt_template.format(
            retrieved_chunks=retrieved_chunks,
            question=prompt
        )

        # Groq çš„ ChatGroq å¯ä»¥ç›´æ¥åƒå­—ä¸²ï¼›é€™è£¡ç¶­æŒä½ åŸæœ¬çš„åšæ³•
        response = llm.invoke(system_prompt + "\n" + final_prompt).content
        st.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
